{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6293732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:248: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:251: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:248: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:251: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\khyal\\AppData\\Local\\Temp\\ipykernel_11008\\2963186169.py:248: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  url_match = re.search('at\\.ua|usa\\.cc|beget\\.tech|16mb\\.com', url)\n",
      "C:\\Users\\khyal\\AppData\\Local\\Temp\\ipykernel_11008\\2963186169.py:251: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  ip_match = re.search('146\\.112\\.61\\.108|23\\.253\\.164\\.', ip_address)\n",
      "c:\\Users\\khyal\\Desktop\\Phishing Detection System\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\khyal\\Desktop\\Phishing Detection System\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 'legitimate', 'phishing_probability': 0.34}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# CELL 1: Setup, Imports, and Helper Functions\n",
    "# ===============================================================\n",
    "\n",
    "import re\n",
    "import socket\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import requests\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "def tokenize_url_words(url: str):\n",
    "    lowered = url.lower()\n",
    "    cleaned = re.sub(r'[/:?=&\\.\\-_~%\\+]+', ' ', lowered)\n",
    "    words = [w for w in cleaned.split() if w]\n",
    "    return words\n",
    "\n",
    "def parse_url_bits(url: str):\n",
    "    parsed = urlparse(url)\n",
    "\n",
    "    scheme = parsed.scheme\n",
    "    netloc = parsed.netloc\n",
    "    path   = parsed.path or \"\"\n",
    "    query  = parsed.query or \"\"\n",
    "\n",
    "    base_url = f\"{scheme}://{netloc}\"\n",
    "    host_only = netloc.split('@')[-1]\n",
    "    host_no_port = host_only.split(':')[0]\n",
    "\n",
    "    tld_info = tldextract.extract(url)\n",
    "    subdomain = tld_info.subdomain or \"\"\n",
    "    domain    = tld_info.domain or \"\"\n",
    "    suffix    = tld_info.suffix or \"\"\n",
    "\n",
    "    full_path_q = path + (\"?\" + query if query else \"\")\n",
    "    words_raw = tokenize_url_words(url)\n",
    "\n",
    "    return {\n",
    "        \"scheme\": scheme,\n",
    "        \"netloc\": netloc,\n",
    "        \"host\": host_no_port,\n",
    "        \"domain\": domain,\n",
    "        \"subdomain\": subdomain,\n",
    "        \"suffix\": suffix,\n",
    "        \"path\": path,\n",
    "        \"query\": query,\n",
    "        \"path_plus_query\": full_path_q,\n",
    "        \"base_url\": base_url,\n",
    "        \"words_raw\": words_raw,\n",
    "        \"url\": url\n",
    "    }\n",
    "\n",
    "def safe_ratio_digits(s: str):\n",
    "    if len(s) == 0:\n",
    "        return 0\n",
    "    return len(re.sub(\"[^0-9]\", \"\", s)) / len(s)\n",
    "\n",
    "with open(\"data/allbrands.txt\", \"r\") as f:\n",
    "    allbrand = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "def brand_in_subdomain(domain_str, subdomain_str):\n",
    "    for b in allbrand:\n",
    "        if b in subdomain_str and b != domain_str:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# CELL 2: Group 1 — Basic URL Structure / Syntax\n",
    "# ===============================================================\n",
    "\n",
    "def extract_group1_basic(url: str): \n",
    "    parts = parse_url_bits(url)\n",
    "\n",
    "    def has_ip(url):\n",
    "        match = re.search(\n",
    "            '(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.'\n",
    "            '([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  # IPv4\n",
    "            '((0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\/)|'  # IPv4 in hexadecimal\n",
    "            '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}|'\n",
    "            '[0-9a-fA-F]{7}', url)  # Ipv6\n",
    "        if match:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def has_prefix_suffix(u):\n",
    "        return 1 if re.search(r\"-\", urlparse(u).netloc.split('.')[-2]) else 0\n",
    "\n",
    "    def has_punycode(u):\n",
    "        return 1 if \"xn--\" in u else 0\n",
    "\n",
    "    def has_tld_in_path(tld, path):\n",
    "        return 1 if tld and tld in path else 0\n",
    "\n",
    "    def has_tld_in_subdomain(tld, subdomain):\n",
    "        return 1 if tld and tld in subdomain else 0\n",
    "\n",
    "    def abnormal_subdomain(u):\n",
    "        sub = urlparse(u).netloc.split('.')\n",
    "        return 1 if len(sub) > 3 else 0\n",
    "\n",
    "    return {\n",
    "        \"length_url\": len(url),\n",
    "        \"length_hostname\": len(parts[\"host\"]),\n",
    "        \"ip\": has_ip(url),\n",
    "        \"nb_dots\": parts[\"host\"].count('.'),\n",
    "        \"port\": 1 if \":\" in parts[\"netloc\"] else 0,\n",
    "        \"ratio_digits_url\": safe_ratio_digits(parts[\"url\"]),\n",
    "        \"ratio_digits_host\": safe_ratio_digits(parts[\"host\"]),\n",
    "        \"punycode\": has_punycode(url),\n",
    "        \"nb_subdomains\": len(parts[\"subdomain\"].split('.')) if parts[\"subdomain\"] else 0,\n",
    "        \"prefix_suffix\": has_prefix_suffix(url),\n",
    "        \"shortening_service\": 1 if re.search(r\"(bit\\.ly|goo\\.gl|tinyurl\\.com|t\\.co|ow\\.ly)\", url) else 0,\n",
    "        \"tld_in_path\": has_tld_in_path(parts[\"suffix\"], parts[\"path_plus_query\"]),\n",
    "        \"tld_in_subdomain\": has_tld_in_subdomain(parts[\"suffix\"], parts[\"subdomain\"]),\n",
    "        \"abnormal_subdomain\": abnormal_subdomain(url),\n",
    "        \"nb_dslash\": url.count(\"//\") - 1,\n",
    "        \"http_in_path\": 1 if \"http\" in parts[\"path_plus_query\"] else 0,\n",
    "        \"https_token\": 1 if \"https\" in parts[\"scheme\"] else 0\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# CELL 3: Group 2 — Symbol Counts and Redirects\n",
    "# ===============================================================\n",
    "\n",
    "def extract_group2_chars(url: str):\n",
    "    parts = parse_url_bits(url)\n",
    "\n",
    "    try:\n",
    "        page = requests.get(url, allow_redirects=True, timeout=3)\n",
    "    except Exception:\n",
    "        class Dummy:\n",
    "            history = []\n",
    "        page = Dummy()\n",
    "\n",
    "    def count_symbol(u, s): return u.count(s)\n",
    "    def count_double_slash(u): return u.count(\"//\") - 1\n",
    "\n",
    "    nb_redirection = len(page.history)\n",
    "    nb_external_redirection = sum(\n",
    "        1 for r in page.history if parts[\"domain\"].lower() not in r.url.lower()\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"nb_hyphens\": count_symbol(url, \"-\"),\n",
    "        \"nb_at\": count_symbol(url, \"@\"),\n",
    "        \"nb_qm\": count_symbol(url, \"?\"),\n",
    "        \"nb_and\": count_symbol(url, \"&\"),\n",
    "        \"nb_or\": url.lower().count(\"|\"),\n",
    "        \"nb_eq\": count_symbol(url, \"=\"),\n",
    "        \"nb_underscore\": count_symbol(url, \"_\"),\n",
    "        \"nb_tilde\": count_symbol(url, '~'),        \n",
    "        \"nb_percent\": count_symbol(url, \"%\"),\n",
    "        \"nb_slash\": count_symbol(url, \"/\"),\n",
    "        \"nb_star\": count_symbol(url, \"*\"),\n",
    "        \"nb_colon\": count_symbol(url, \":\"),\n",
    "        \"nb_comma\": count_symbol(url, \",\"),\n",
    "        \"nb_semicolumn\": count_symbol(url, \";\"),\n",
    "        \"nb_dollar\": count_symbol(url, \"$\"),\n",
    "        \"nb_space\": count_symbol(url, \" \"),\n",
    "        \"nb_www\": 1 if \"www\" in url.lower() else 0,\n",
    "        \"nb_com\": url.lower().count(\".com\"),\n",
    "        \"nb_dslash\": count_double_slash(url),\n",
    "        \"nb_redirection\": nb_redirection,\n",
    "        \"nb_external_redirection\": nb_external_redirection,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# CELL 4: Group 3 — Word Statistics\n",
    "# ===============================================================\n",
    "\n",
    "def extract_group3_wordstats(url: str):\n",
    "    parts = parse_url_bits(url)\n",
    "    words = parts[\"words_raw\"]\n",
    "\n",
    "    def length_words_raw(words): return sum(len(w) for w in words)\n",
    "    def char_repeat(words): return max([words.count(w) for w in set(words)]) if words else 0\n",
    "    def shortest(words): return min(map(len, words)) if words else 0\n",
    "    def longest(words): return max(map(len, words)) if words else 0\n",
    "    def average(words): return sum(map(len, words)) / len(words) if words else 0\n",
    "\n",
    "    host = [parts[\"host\"]]\n",
    "    path = [parts[\"path\"] or \"\"]\n",
    "\n",
    "    return {\n",
    "        \"length_words_raw\": length_words_raw(words),\n",
    "        \"char_repeat\": char_repeat(words),\n",
    "        \"shortest_words_raw\": shortest(words),\n",
    "        \"shortest_word_host\": shortest(host),\n",
    "        \"shortest_word_path\": shortest(path),\n",
    "        \"longest_words_raw\": longest(words),\n",
    "        \"longest_word_host\": longest(host),\n",
    "        \"longest_word_path\": longest(path),\n",
    "        \"avg_words_raw\": average(words),\n",
    "        \"avg_word_host\": average(host),\n",
    "        \"avg_word_path\": average(path),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# CELL 5: Group 4 — Phishing / Brand / DNS / TLD Heuristics\n",
    "# ===============================================================\n",
    "\n",
    "def extract_group4_phish(url: str):\n",
    "    parts = parse_url_bits(url)\n",
    "\n",
    "    def phish_hints(path_q):\n",
    "        hints = ['wp', 'login', 'includes', 'admin', 'content', 'site', 'images', 'js', 'alibaba', 'css', 'myaccount', 'dropbox', 'themes', 'plugins', 'signin', 'view']\n",
    "        return 1 if any(h in path_q.lower() for h in hints) else 0\n",
    "\n",
    "    def domain_in_brand(domain):\n",
    "        return 1 if domain in allbrand else 0\n",
    "\n",
    "    def brand_in_path(domain, path_q):\n",
    "        for b in allbrand:\n",
    "            if b in path_q and b != domain:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def suspecious_tld(tld):\n",
    "        bad_tlds = [\"zip\", \"xyz\", \"top\", \"gq\", \"tk\", \"ml\", \"cf\"]\n",
    "        return 1 if tld in bad_tlds else 0\n",
    "\n",
    "    def statistical_report(url, domain):\n",
    "        url_match = re.search('at\\.ua|usa\\.cc|beget\\.tech|16mb\\.com', url)\n",
    "        try:\n",
    "            ip_address = socket.gethostbyname(domain)\n",
    "            ip_match = re.search('146\\.112\\.61\\.108|23\\.253\\.164\\.', ip_address)\n",
    "            if url_match or ip_match:\n",
    "                return 1\n",
    "        except:\n",
    "            pass\n",
    "        return 0\n",
    "\n",
    "    return {\n",
    "        \"phish_hints\": phish_hints(parts[\"path_plus_query\"]),\n",
    "        \"domain_in_brand\": domain_in_brand(parts[\"domain\"]),\n",
    "        \"brand_in_subdomain\": brand_in_subdomain(parts[\"domain\"], parts[\"subdomain\"]),\n",
    "        \"brand_in_path\": brand_in_path(parts[\"domain\"], parts[\"path_plus_query\"]),\n",
    "        \"suspecious_tld\": suspecious_tld(parts[\"suffix\"]),\n",
    "        \"statistical_report\": statistical_report(parts[\"url\"], parts[\"domain\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# CELL 6: Combine All Feature Groups into One Extractor\n",
    "# ===============================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# helper function (used for random_domain feature)\n",
    "# ===============================================================\n",
    "def check_word_random(domain: str) -> int:\n",
    "    d = domain.lower()\n",
    "    if re.search(r\"\\d\", d):\n",
    "        return 1\n",
    "    if re.search(r\"[bcdfghjklmnpqrstvwxyz]{4,}\", d):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Combine All Feature Groups into One Extractor\n",
    "# ===============================================================\n",
    "def extract_all_url_structure_features(url: str):\n",
    "    g1 = extract_group1_basic(url)\n",
    "    g2 = extract_group2_chars(url)\n",
    "    g3 = extract_group3_wordstats(url)\n",
    "    g4 = extract_group4_phish(url)\n",
    "\n",
    "    parts = parse_url_bits(url)\n",
    "\n",
    "    random_val = check_word_random(parts[\"domain\"])\n",
    "\n",
    "    features = OrderedDict([\n",
    "        (\"length_url\", g1[\"length_url\"]),\n",
    "        (\"length_hostname\", g1[\"length_hostname\"]),\n",
    "        (\"ip\", g1[\"ip\"]),\n",
    "        (\"nb_dots\", g1[\"nb_dots\"]),\n",
    "        (\"nb_hyphens\", g2[\"nb_hyphens\"]),\n",
    "        (\"nb_at\", g2[\"nb_at\"]),\n",
    "        (\"nb_qm\", g2[\"nb_qm\"]),\n",
    "        (\"nb_and\", g2[\"nb_and\"]),\n",
    "        (\"nb_or\", g2[\"nb_or\"]),\n",
    "        (\"nb_eq\", g2[\"nb_eq\"]),\n",
    "        (\"nb_underscore\", g2[\"nb_underscore\"]),\n",
    "        (\"nb_tilde\", g2[\"nb_tilde\"]),\n",
    "        (\"nb_percent\", g2[\"nb_percent\"]),\n",
    "        (\"nb_slash\", g2[\"nb_slash\"]),\n",
    "        (\"nb_star\", g2[\"nb_star\"]),\n",
    "        (\"nb_colon\", g2[\"nb_colon\"]),\n",
    "        (\"nb_comma\", g2[\"nb_comma\"]),\n",
    "        (\"nb_semicolumn\", g2[\"nb_semicolumn\"]),\n",
    "        (\"nb_dollar\", g2[\"nb_dollar\"]),\n",
    "        (\"nb_space\", g2[\"nb_space\"]),\n",
    "        (\"nb_www\", g2[\"nb_www\"]),\n",
    "        (\"nb_com\", g2[\"nb_com\"]),\n",
    "        (\"nb_dslash\", g1[\"nb_dslash\"]),\n",
    "        (\"http_in_path\", g1[\"http_in_path\"]),\n",
    "        (\"https_token\", g1[\"https_token\"]),\n",
    "        (\"ratio_digits_url\", g1[\"ratio_digits_url\"]),\n",
    "        (\"ratio_digits_host\", g1[\"ratio_digits_host\"]),\n",
    "        (\"punycode\", g1[\"punycode\"]),\n",
    "        (\"port\", g1[\"port\"]),\n",
    "        (\"tld_in_path\", g1[\"tld_in_path\"]),\n",
    "        (\"tld_in_subdomain\", g1[\"tld_in_subdomain\"]),\n",
    "        (\"abnormal_subdomain\", g1[\"abnormal_subdomain\"]),\n",
    "        (\"nb_subdomains\", g1[\"nb_subdomains\"]),\n",
    "        (\"prefix_suffix\", g1[\"prefix_suffix\"]),\n",
    "        (\"random_domain\", random_val),\n",
    "        (\"shortening_service\", g1[\"shortening_service\"]),\n",
    "        (\"path_extension\", 1 if \".\" in parts[\"path\"].split(\"/\")[-1] else 0),\n",
    "        (\"nb_redirection\", g2[\"nb_redirection\"]),\n",
    "        (\"nb_external_redirection\", g2[\"nb_external_redirection\"]),\n",
    "        (\"length_words_raw\", g3[\"length_words_raw\"]),\n",
    "        (\"char_repeat\", g3[\"char_repeat\"]),\n",
    "        (\"shortest_words_raw\", g3[\"shortest_words_raw\"]),\n",
    "        (\"shortest_word_host\", g3[\"shortest_word_host\"]),\n",
    "        (\"shortest_word_path\", g3[\"shortest_word_path\"]),\n",
    "        (\"longest_words_raw\", g3[\"longest_words_raw\"]),\n",
    "        (\"longest_word_host\", g3[\"longest_word_host\"]),\n",
    "        (\"longest_word_path\", g3[\"longest_word_path\"]),\n",
    "        (\"avg_words_raw\", g3[\"avg_words_raw\"]),\n",
    "        (\"avg_word_host\", g3[\"avg_word_host\"]),\n",
    "        (\"avg_word_path\", g3[\"avg_word_path\"]),\n",
    "        (\"phish_hints\", g4[\"phish_hints\"]),\n",
    "        (\"domain_in_brand\", g4[\"domain_in_brand\"]),\n",
    "        (\"brand_in_subdomain\", g4[\"brand_in_subdomain\"]),\n",
    "        (\"brand_in_path\", g4[\"brand_in_path\"]),\n",
    "        (\"suspecious_tld\", g4[\"suspecious_tld\"]),\n",
    "        (\"statistical_report\", g4[\"statistical_report\"]),\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# CELL 7: Testing the Extractor\n",
    "# ===============================================================\n",
    "\n",
    "test_urls = [\n",
    "    \"http://rgipt.ac.in\"\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for u in test_urls:\n",
    "    # print(\"\\nURL:\", u)\n",
    "    features = extract_all_url_structure_features(u)\n",
    "    features_dict = dict(features)  # convert OrderedDict → normal dict\n",
    "    # print(f\"Extracted {len(features_dict)} features.\\n\")\n",
    "    results[u] = features_dict  \n",
    "feature_input = results[test_urls[0]]  # features for the first test URL\n",
    "feature_input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_url_features(feature_dict_or_series, artifacts_path=\"phish_artifacts.pkl\"):\n",
    "    bundle = joblib.load(artifacts_path)\n",
    "    selected_cols = bundle[\"selected_cols\"]\n",
    "    scaler       = bundle[\"scaler\"]\n",
    "    best_model   = bundle[\"best_model\"]\n",
    "    best_scaled  = bundle[\"best_scaled\"]\n",
    "    name         = bundle[\"best_model_name\"]\n",
    " \n",
    "    X_new = pd.DataFrame([dict(feature_dict_or_series)])\n",
    "\n",
    "    X_new = X_new.reindex(columns=selected_cols, fill_value=0)\n",
    "\n",
    "    if best_scaled:\n",
    "        X_new_arr = scaler.transform(X_new)\n",
    "    else:\n",
    "        X_new_arr = X_new.values\n",
    "\n",
    "    y_pred = best_model.predict(X_new_arr)[0]\n",
    "\n",
    "    try:\n",
    "        y_proba = best_model.predict_proba(X_new_arr)[:, 1][0]\n",
    "    except Exception:\n",
    "        y_proba = (best_model.decision_function(X_new_arr)[0]\n",
    "                   if hasattr(best_model, \"decision_function\") else np.nan)\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"label\": y_pred,\n",
    "        \"phishing_probability\": float(y_proba) if np.isscalar(y_proba) else np.nan\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_features = feature_input  # Use the features extracted earlier\n",
    "result = predict_url_features(new_features)\n",
    "\n",
    "\n",
    "required_result = {\n",
    "    'label': result['label'],\n",
    "    'phishing_probability': round(result['phishing_probability'], 2)\n",
    "}\n",
    "required_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b98184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be90dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cddf91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# PSEUDOCODE TEMPLATE — PHISHING URL DETECTOR\n",
    "# ===============================================================\n",
    "\n",
    "# ---------- Import necessary libraries ----------\n",
    "# import pandas as pd\n",
    "# import joblib\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1814d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ---------- Define Feature Extraction Function ----------\n",
    "# def extract(url):\n",
    "#     \"\"\"\n",
    "#     Extract all numerical features from the input URL.\n",
    "#     Input:\n",
    "#         url (str): The website URL to analyze\n",
    "#     Returns:\n",
    "#         features (dict): Extracted numeric features for the model\n",
    "#     \"\"\"\n",
    "#     # TODO: Implement your feature extraction logic here\n",
    "#     # Example:\n",
    "#     # features = extract_all_url_structure_features(url)\n",
    "#     # return features\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ---------- Define Prediction Function ----------\n",
    "# def predict(features):\n",
    "#     \"\"\"\n",
    "#     Use the trained model to predict phishing probability.\n",
    "#     Input:\n",
    "#         features (dict): Extracted features from URL\n",
    "#     Returns:\n",
    "#         result (str): 'phishing' or 'legitimate'\n",
    "#         confidence (float): Probability or confidence score\n",
    "#     \"\"\"\n",
    "#     # TODO:\n",
    "#     # 1. Align feature dict to training columns\n",
    "#     # 2. Apply scaling if required\n",
    "#     # 3. Load trained model (if not in memory)\n",
    "#     # 4. Predict class and probability\n",
    "#     # return result, confidence\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8960347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # ---------- Main Function ----------\n",
    "# def main():\n",
    "#     \"\"\"\n",
    "#     Entry point of the program.\n",
    "#     1. Accept URL input\n",
    "#     2. Extract features\n",
    "#     3. Get prediction and confidence score\n",
    "#     4. Print or return results\n",
    "#     \"\"\"\n",
    "#     # Example:\n",
    "#     url = \"hjcbnhdfbhvgjnsjmjkjv\"  # Input URL\n",
    "#     features = extract(url)\n",
    "#     result, confidence = predict(features)\n",
    "#     print(f\"URL: {url}\")\n",
    "#     print(f\"Prediction: {result} (Confidence: {confidence:.2f})\")\n",
    "\n",
    "# # ---------- Run the main function ----------\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ca133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4cb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caeaf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c4f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185e7d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667246c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef526049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
