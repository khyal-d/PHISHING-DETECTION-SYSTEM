{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa93f496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab32e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CELL 1: Setup, Imports, and Helper Functions\n",
    "# ===============================================================\n",
    "\n",
    "import re\n",
    "import socket\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import requests\n",
    "\n",
    "# --- Tokenize URL into \"words\" for word-level statistics\n",
    "def tokenize_url_words(url: str):\n",
    "    lowered = url.lower()\n",
    "    cleaned = re.sub(r'[/:?=&\\.\\-_~%\\+]+', ' ', lowered)\n",
    "    words = [w for w in cleaned.split() if w]\n",
    "    return words\n",
    "\n",
    "# --- Extract key URL parts (scheme, host, domain, path, etc.)\n",
    "def parse_url_bits(url: str):\n",
    "    parsed = urlparse(url)\n",
    "\n",
    "    scheme = parsed.scheme\n",
    "    netloc = parsed.netloc\n",
    "    path   = parsed.path or \"\"\n",
    "    query  = parsed.query or \"\"\n",
    "\n",
    "    base_url = f\"{scheme}://{netloc}\"\n",
    "    host_only = netloc.split('@')[-1]\n",
    "    host_no_port = host_only.split(':')[0]\n",
    "\n",
    "    tld_info = tldextract.extract(url)\n",
    "    subdomain = tld_info.subdomain or \"\"\n",
    "    domain    = tld_info.domain or \"\"\n",
    "    suffix    = tld_info.suffix or \"\"\n",
    "\n",
    "    full_path_q = path + (\"?\" + query if query else \"\")\n",
    "    words_raw = tokenize_url_words(url)\n",
    "\n",
    "    return {\n",
    "        \"scheme\": scheme,\n",
    "        \"netloc\": netloc,\n",
    "        \"host\": host_no_port,\n",
    "        \"domain\": domain,\n",
    "        \"subdomain\": subdomain,\n",
    "        \"suffix\": suffix,\n",
    "        \"path\": path,\n",
    "        \"query\": query,\n",
    "        \"path_plus_query\": full_path_q,\n",
    "        \"base_url\": base_url,\n",
    "        \"words_raw\": words_raw,\n",
    "        \"url\": url\n",
    "    }\n",
    "\n",
    "# --- Safe digit ratio helper\n",
    "def safe_ratio_digits(s: str):\n",
    "    if len(s) == 0:\n",
    "        return 0\n",
    "    return len(re.sub(\"[^0-9]\", \"\", s)) / len(s)\n",
    "\n",
    "# --- Brand check in subdomain (requires allbrands.txt)\n",
    "with open(\"data/allbrands.txt\", \"r\") as f:\n",
    "    allbrand = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "def brand_in_subdomain(domain_str, subdomain_str):\n",
    "    for b in allbrand:\n",
    "        if b in subdomain_str and b != domain_str:\n",
    "            return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64570a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c8052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CELL 2: Group 1 — Basic URL Structure / Syntax\n",
    "# ===============================================================\n",
    "\n",
    "def extract_group1_basic(url: str): \n",
    "    parts = parse_url_bits(url)\n",
    "\n",
    "    # def has_ip(u):\n",
    "    #     pattern = re.compile(r'(\\d{1,3}\\.){3}\\d{1,3}')\n",
    "    #     return 1 if pattern.search(u) else 0\n",
    "    \n",
    "\n",
    "    def has_ip(url):\n",
    "        match = re.search(\n",
    "            '(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.'\n",
    "            '([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  # IPv4\n",
    "            '((0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\/)|'  # IPv4 in hexadecimal\n",
    "            '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}|'\n",
    "            '[0-9a-fA-F]{7}', url)  # Ipv6\n",
    "        if match:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def has_prefix_suffix(u):\n",
    "        return 1 if re.search(r\"-\", urlparse(u).netloc.split('.')[-2]) else 0\n",
    "\n",
    "    def has_punycode(u):\n",
    "        return 1 if \"xn--\" in u else 0\n",
    "\n",
    "    def has_tld_in_path(tld, path):\n",
    "        return 1 if tld and tld in path else 0\n",
    "\n",
    "    def has_tld_in_subdomain(tld, subdomain):\n",
    "        return 1 if tld and tld in subdomain else 0\n",
    "\n",
    "    def abnormal_subdomain(u):\n",
    "        sub = urlparse(u).netloc.split('.')\n",
    "        return 1 if len(sub) > 3 else 0\n",
    "\n",
    "    return {\n",
    "        \"length_url\": len(url),\n",
    "        \"length_hostname\": len(parts[\"host\"]),\n",
    "        \"ip\": has_ip(url),\n",
    "        \"nb_dots\": parts[\"host\"].count('.'),\n",
    "        \"port\": 1 if \":\" in parts[\"netloc\"] else 0,\n",
    "        \"ratio_digits_url\": safe_ratio_digits(parts[\"url\"]),\n",
    "        \"ratio_digits_host\": safe_ratio_digits(parts[\"host\"]),\n",
    "        \"punycode\": has_punycode(url),\n",
    "        \"nb_subdomains\": len(parts[\"subdomain\"].split('.')) if parts[\"subdomain\"] else 0,\n",
    "        \"prefix_suffix\": has_prefix_suffix(url),\n",
    "        \"shortening_service\": 1 if re.search(r\"(bit\\.ly|goo\\.gl|tinyurl\\.com|t\\.co|ow\\.ly)\", url) else 0,\n",
    "        \"tld_in_path\": has_tld_in_path(parts[\"suffix\"], parts[\"path_plus_query\"]),\n",
    "        \"tld_in_subdomain\": has_tld_in_subdomain(parts[\"suffix\"], parts[\"subdomain\"]),\n",
    "        \"abnormal_subdomain\": abnormal_subdomain(url),\n",
    "        \"nb_dslash\": url.count(\"//\") - 1,\n",
    "        \"http_in_path\": 1 if \"http\" in parts[\"path_plus_query\"] else 0,\n",
    "        \"https_token\": 1 if \"https\" in parts[\"scheme\"] else 0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287fdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db0b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CELL 3: Group 2 — Symbol Counts and Redirects\n",
    "# ===============================================================\n",
    "\n",
    "def extract_group2_chars(url: str):\n",
    "    parts = parse_url_bits(url)\n",
    "\n",
    "    # attempt HTTP GET for redirect info\n",
    "    try:\n",
    "        page = requests.get(url, allow_redirects=True, timeout=3)\n",
    "    except Exception:\n",
    "        class Dummy:\n",
    "            history = []\n",
    "        page = Dummy()\n",
    "\n",
    "    def count_symbol(u, s): return u.count(s)\n",
    "    def count_double_slash(u): return u.count(\"//\") - 1\n",
    "\n",
    "    nb_redirection = len(page.history)\n",
    "    nb_external_redirection = sum(\n",
    "        1 for r in page.history if parts[\"domain\"].lower() not in r.url.lower()\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"nb_hyphens\": count_symbol(url, \"-\"),\n",
    "        \"nb_at\": count_symbol(url, \"@\"),\n",
    "        \"nb_qm\": count_symbol(url, \"?\"),\n",
    "        \"nb_and\": count_symbol(url, \"&\"),\n",
    "        \"nb_or\": url.lower().count(\"|\"),\n",
    "        \"nb_eq\": count_symbol(url, \"=\"),\n",
    "        \"nb_underscore\": count_symbol(url, \"_\"),\n",
    "        \"nb_tilde\": count_symbol(url, '~'),\n",
    "        \n",
    "#################################################################################################################################\n",
    "#              Checks if tilde symbol exist in webpage URL \n",
    "#################################################################################################################################\n",
    "\n",
    "# def count_tilde(full_url):\n",
    "#     if full_url.count('~')>0:\n",
    "#         return 1\n",
    "#     return 0\n",
    "    \n",
    "        \"nb_percent\": count_symbol(url, \"%\"),\n",
    "        \"nb_slash\": count_symbol(url, \"/\"),\n",
    "        \"nb_star\": count_symbol(url, \"*\"),\n",
    "        \"nb_colon\": count_symbol(url, \":\"),\n",
    "        \"nb_comma\": count_symbol(url, \",\"),\n",
    "        \"nb_semicolumn\": count_symbol(url, \";\"),\n",
    "        \"nb_dollar\": count_symbol(url, \"$\"),\n",
    "        \"nb_space\": count_symbol(url, \" \"),\n",
    "        \"nb_www\": 1 if \"www\" in url.lower() else 0,\n",
    "        \"nb_com\": url.lower().count(\".com\"),\n",
    "        \"nb_dslash\": count_double_slash(url),\n",
    "        \"nb_redirection\": nb_redirection,\n",
    "        \"nb_external_redirection\": nb_external_redirection,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eca3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e48609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CELL 4: Group 3 — Word Statistics\n",
    "# ===============================================================\n",
    "\n",
    "def extract_group3_wordstats(url: str):\n",
    "    parts = parse_url_bits(url)\n",
    "    words = parts[\"words_raw\"]\n",
    "\n",
    "    def length_words_raw(words): return sum(len(w) for w in words)\n",
    "    def char_repeat(words): return max([words.count(w) for w in set(words)]) if words else 0\n",
    "    def shortest(words): return min(map(len, words)) if words else 0\n",
    "    def longest(words): return max(map(len, words)) if words else 0\n",
    "    def average(words): return sum(map(len, words)) / len(words) if words else 0\n",
    "\n",
    "    host = [parts[\"host\"]]\n",
    "    path = [parts[\"path\"] or \"\"]\n",
    "\n",
    "    return {\n",
    "        \"length_words_raw\": length_words_raw(words),\n",
    "        \"char_repeat\": char_repeat(words),\n",
    "        \"shortest_words_raw\": shortest(words),\n",
    "        \"shortest_word_host\": shortest(host),\n",
    "        \"shortest_word_path\": shortest(path),\n",
    "        \"longest_words_raw\": longest(words),\n",
    "        \"longest_word_host\": longest(host),\n",
    "        \"longest_word_path\": longest(path),\n",
    "        \"avg_words_raw\": average(words),\n",
    "        \"avg_word_host\": average(host),\n",
    "        \"avg_word_path\": average(path),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a59826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436da2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:26: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\khyal\\AppData\\Local\\Temp\\ipykernel_2456\\683812489.py:26: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  url_match = re.search('at\\.ua|usa\\.cc|beget\\.tech|16mb\\.com', url)\n",
      "C:\\Users\\khyal\\AppData\\Local\\Temp\\ipykernel_2456\\683812489.py:29: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  ip_match = re.search('146\\.112\\.61\\.108|23\\.253\\.164\\.', ip_address)\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# CELL 5: Group 4 — Phishing / Brand / DNS / TLD Heuristics\n",
    "# ===============================================================\n",
    "\n",
    "def extract_group4_phish(url: str):\n",
    "    parts = parse_url_bits(url)\n",
    "\n",
    "    def phish_hints(path_q):\n",
    "        hints = ['wp', 'login', 'includes', 'admin', 'content', 'site', 'images', 'js', 'alibaba', 'css', 'myaccount', 'dropbox', 'themes', 'plugins', 'signin', 'view']\n",
    "        return 1 if any(h in path_q.lower() for h in hints) else 0\n",
    "\n",
    "    def domain_in_brand(domain):\n",
    "        return 1 if domain in allbrand else 0\n",
    "\n",
    "    def brand_in_path(domain, path_q):\n",
    "        for b in allbrand:\n",
    "            if b in path_q and b != domain:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def suspecious_tld(tld):\n",
    "        bad_tlds = [\"zip\", \"xyz\", \"top\", \"gq\", \"tk\", \"ml\", \"cf\"]\n",
    "        return 1 if tld in bad_tlds else 0\n",
    "\n",
    "    def statistical_report(url, domain):\n",
    "        url_match = re.search('at\\.ua|usa\\.cc|beget\\.tech|16mb\\.com', url)\n",
    "        try:\n",
    "            ip_address = socket.gethostbyname(domain)\n",
    "            ip_match = re.search('146\\.112\\.61\\.108|23\\.253\\.164\\.', ip_address)\n",
    "            if url_match or ip_match:\n",
    "                return 1\n",
    "        except:\n",
    "            pass\n",
    "        return 0\n",
    "\n",
    "    return {\n",
    "        \"phish_hints\": phish_hints(parts[\"path_plus_query\"]),\n",
    "        \"domain_in_brand\": domain_in_brand(parts[\"domain\"]),\n",
    "        \"brand_in_subdomain\": brand_in_subdomain(parts[\"domain\"], parts[\"subdomain\"]),\n",
    "        \"brand_in_path\": brand_in_path(parts[\"domain\"], parts[\"path_plus_query\"]),\n",
    "        \"suspecious_tld\": suspecious_tld(parts[\"suffix\"]),\n",
    "        \"statistical_report\": statistical_report(parts[\"url\"], parts[\"domain\"]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27104f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "481f83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CELL 6: Combine All Feature Groups into One Extractor\n",
    "# ===============================================================\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# NLP helper function (used for random_domain feature)\n",
    "# ===============================================================\n",
    "def check_word_random(domain: str) -> int:\n",
    "    \"\"\"\n",
    "    Minimal heuristic for detecting random-looking domains.\n",
    "    Returns 1 if the domain seems random, else 0.\n",
    "    \"\"\"\n",
    "    d = domain.lower()\n",
    "\n",
    "    # Heuristic 1: contains digits\n",
    "    if re.search(r\"\\d\", d):\n",
    "        return 1\n",
    "\n",
    "    # Heuristic 2: too many consonants in a row (>=4)\n",
    "    if re.search(r\"[bcdfghjklmnpqrstvwxyz]{4,}\", d):\n",
    "        return 1\n",
    "\n",
    "    # Looks normal\n",
    "    return 0\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Combine All Feature Groups into One Extractor\n",
    "# ===============================================================\n",
    "def extract_all_url_structure_features(url: str):\n",
    "    g1 = extract_group1_basic(url)\n",
    "    g2 = extract_group2_chars(url)\n",
    "    g3 = extract_group3_wordstats(url)\n",
    "    g4 = extract_group4_phish(url)\n",
    "\n",
    "    parts = parse_url_bits(url)\n",
    "\n",
    "    # Use the local function instead of a class\n",
    "    random_val = check_word_random(parts[\"domain\"])\n",
    "\n",
    "    features = OrderedDict([\n",
    "        (\"length_url\", g1[\"length_url\"]),\n",
    "        (\"length_hostname\", g1[\"length_hostname\"]),\n",
    "        (\"ip\", g1[\"ip\"]),\n",
    "        (\"nb_dots\", g1[\"nb_dots\"]),\n",
    "        (\"nb_hyphens\", g2[\"nb_hyphens\"]),\n",
    "        (\"nb_at\", g2[\"nb_at\"]),\n",
    "        (\"nb_qm\", g2[\"nb_qm\"]),\n",
    "        (\"nb_and\", g2[\"nb_and\"]),\n",
    "        (\"nb_or\", g2[\"nb_or\"]),\n",
    "        (\"nb_eq\", g2[\"nb_eq\"]),\n",
    "        (\"nb_underscore\", g2[\"nb_underscore\"]),\n",
    "        (\"nb_tilde\", g2[\"nb_tilde\"]),\n",
    "        (\"nb_percent\", g2[\"nb_percent\"]),\n",
    "        (\"nb_slash\", g2[\"nb_slash\"]),\n",
    "        (\"nb_star\", g2[\"nb_star\"]),\n",
    "        (\"nb_colon\", g2[\"nb_colon\"]),\n",
    "        (\"nb_comma\", g2[\"nb_comma\"]),\n",
    "        (\"nb_semicolumn\", g2[\"nb_semicolumn\"]),\n",
    "        (\"nb_dollar\", g2[\"nb_dollar\"]),\n",
    "        (\"nb_space\", g2[\"nb_space\"]),\n",
    "        (\"nb_www\", g2[\"nb_www\"]),\n",
    "        (\"nb_com\", g2[\"nb_com\"]),\n",
    "        (\"nb_dslash\", g1[\"nb_dslash\"]),\n",
    "        (\"http_in_path\", g1[\"http_in_path\"]),\n",
    "        (\"https_token\", g1[\"https_token\"]),\n",
    "        (\"ratio_digits_url\", g1[\"ratio_digits_url\"]),\n",
    "        (\"ratio_digits_host\", g1[\"ratio_digits_host\"]),\n",
    "        (\"punycode\", g1[\"punycode\"]),\n",
    "        (\"port\", g1[\"port\"]),\n",
    "        (\"tld_in_path\", g1[\"tld_in_path\"]),\n",
    "        (\"tld_in_subdomain\", g1[\"tld_in_subdomain\"]),\n",
    "        (\"abnormal_subdomain\", g1[\"abnormal_subdomain\"]),\n",
    "        (\"nb_subdomains\", g1[\"nb_subdomains\"]),\n",
    "        (\"prefix_suffix\", g1[\"prefix_suffix\"]),\n",
    "        (\"random_domain\", random_val),\n",
    "        (\"shortening_service\", g1[\"shortening_service\"]),\n",
    "        (\"path_extension\", 1 if \".\" in parts[\"path\"].split(\"/\")[-1] else 0),\n",
    "        (\"nb_redirection\", g2[\"nb_redirection\"]),\n",
    "        (\"nb_external_redirection\", g2[\"nb_external_redirection\"]),\n",
    "        (\"length_words_raw\", g3[\"length_words_raw\"]),\n",
    "        (\"char_repeat\", g3[\"char_repeat\"]),\n",
    "        (\"shortest_words_raw\", g3[\"shortest_words_raw\"]),\n",
    "        (\"shortest_word_host\", g3[\"shortest_word_host\"]),\n",
    "        (\"shortest_word_path\", g3[\"shortest_word_path\"]),\n",
    "        (\"longest_words_raw\", g3[\"longest_words_raw\"]),\n",
    "        (\"longest_word_host\", g3[\"longest_word_host\"]),\n",
    "        (\"longest_word_path\", g3[\"longest_word_path\"]),\n",
    "        (\"avg_words_raw\", g3[\"avg_words_raw\"]),\n",
    "        (\"avg_word_host\", g3[\"avg_word_host\"]),\n",
    "        (\"avg_word_path\", g3[\"avg_word_path\"]),\n",
    "        (\"phish_hints\", g4[\"phish_hints\"]),\n",
    "        (\"domain_in_brand\", g4[\"domain_in_brand\"]),\n",
    "        (\"brand_in_subdomain\", g4[\"brand_in_subdomain\"]),\n",
    "        (\"brand_in_path\", g4[\"brand_in_path\"]),\n",
    "        (\"suspecious_tld\", g4[\"suspecious_tld\"]),\n",
    "        (\"statistical_report\", g4[\"statistical_report\"]),\n",
    "    ])\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd08d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8fe4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===============================================================\n",
    "# # CELL 7: Testing the Extractor\n",
    "# # ===============================================================\n",
    "\n",
    "# test_urls = [\n",
    "#     \"https://support-appleld.com.secureupdate.duilawyeryork.com/ap/89e6a3b4b063b8d/?cmd=_update&dispatch=89e6a3b4b063b8d1b&locale=_\"\n",
    "# ]\n",
    "\n",
    "# for u in test_urls:\n",
    "#     print(\"\\nURL:\", u)\n",
    "#     features = extract_all_url_structure_features(u)\n",
    "#     print(f\"Extracted {len(features)} features:\")\n",
    "#     features\n",
    "# features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa3327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb402513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "URL: http://rgipt.ac.in\n",
      "Extracted 56 features.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'length_url': 18,\n",
       " 'length_hostname': 11,\n",
       " 'ip': 0,\n",
       " 'nb_dots': 2,\n",
       " 'nb_hyphens': 0,\n",
       " 'nb_at': 0,\n",
       " 'nb_qm': 0,\n",
       " 'nb_and': 0,\n",
       " 'nb_or': 0,\n",
       " 'nb_eq': 0,\n",
       " 'nb_underscore': 0,\n",
       " 'nb_tilde': 0,\n",
       " 'nb_percent': 0,\n",
       " 'nb_slash': 2,\n",
       " 'nb_star': 0,\n",
       " 'nb_colon': 1,\n",
       " 'nb_comma': 0,\n",
       " 'nb_semicolumn': 0,\n",
       " 'nb_dollar': 0,\n",
       " 'nb_space': 0,\n",
       " 'nb_www': 0,\n",
       " 'nb_com': 0,\n",
       " 'nb_dslash': 0,\n",
       " 'http_in_path': 0,\n",
       " 'https_token': 0,\n",
       " 'ratio_digits_url': 0.0,\n",
       " 'ratio_digits_host': 0.0,\n",
       " 'punycode': 0,\n",
       " 'port': 0,\n",
       " 'tld_in_path': 0,\n",
       " 'tld_in_subdomain': 0,\n",
       " 'abnormal_subdomain': 0,\n",
       " 'nb_subdomains': 0,\n",
       " 'prefix_suffix': 0,\n",
       " 'random_domain': 0,\n",
       " 'shortening_service': 0,\n",
       " 'path_extension': 0,\n",
       " 'nb_redirection': 1,\n",
       " 'nb_external_redirection': 0,\n",
       " 'length_words_raw': 13,\n",
       " 'char_repeat': 1,\n",
       " 'shortest_words_raw': 2,\n",
       " 'shortest_word_host': 11,\n",
       " 'shortest_word_path': 0,\n",
       " 'longest_words_raw': 5,\n",
       " 'longest_word_host': 11,\n",
       " 'longest_word_path': 0,\n",
       " 'avg_words_raw': 3.25,\n",
       " 'avg_word_host': 11.0,\n",
       " 'avg_word_path': 0.0,\n",
       " 'phish_hints': 0,\n",
       " 'domain_in_brand': 0,\n",
       " 'brand_in_subdomain': 0,\n",
       " 'brand_in_path': 0,\n",
       " 'suspecious_tld': 0,\n",
       " 'statistical_report': 0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# CELL 7: Testing the Extractor\n",
    "# ===============================================================\n",
    "\n",
    "test_urls = [\n",
    "    \"http://rgipt.ac.in\"\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for u in test_urls:\n",
    "    print(\"\\nURL:\", u)\n",
    "    features = extract_all_url_structure_features(u)\n",
    "    features_dict = dict(features)  # convert OrderedDict → normal dict\n",
    "    print(f\"Extracted {len(features_dict)} features.\\n\")\n",
    "    results[u] = features_dict  # store result in a dictionary\n",
    "\n",
    "# Now you can access the result like:\n",
    "feature_input = results[test_urls[0]]  # features for the first test URL\n",
    "feature_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebab3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131bbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2706d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d4189a3",
   "metadata": {},
   "source": [
    "# Fetaure iput and model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96615a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4dad7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "def predict_url_features(feature_dict_or_series, artifacts_path=\"phish_artifacts.pkl\"):\n",
    "    # Load artifacts\n",
    "    bundle = joblib.load(artifacts_path)\n",
    "    selected_cols = bundle[\"selected_cols\"]\n",
    "    scaler       = bundle[\"scaler\"]\n",
    "    best_model   = bundle[\"best_model\"]\n",
    "    best_scaled  = bundle[\"best_scaled\"]\n",
    "    name         = bundle[\"best_model_name\"]\n",
    " \n",
    "    # Build one-row DataFrame from the extracted features (56 keys)\n",
    "    X_new = pd.DataFrame([dict(feature_dict_or_series)])\n",
    "\n",
    "    # Align to training-time selection & order:\n",
    "    # - keep only the selected columns\n",
    "    # - add any missing ones as 0 (or another default if you prefer)\n",
    "    X_new = X_new.reindex(columns=selected_cols, fill_value=0)\n",
    "\n",
    "    # Apply the same scaler if the model expects scaled inputs\n",
    "    if best_scaled:\n",
    "        X_new_arr = scaler.transform(X_new)\n",
    "    else:\n",
    "        X_new_arr = X_new.values\n",
    "\n",
    "    # Predict label\n",
    "    y_pred = best_model.predict(X_new_arr)[0]\n",
    "\n",
    "    # Probability if supported\n",
    "    try:\n",
    "        y_proba = best_model.predict_proba(X_new_arr)[:, 1][0]\n",
    "    except Exception:\n",
    "        y_proba = (best_model.decision_function(X_new_arr)[0]\n",
    "                   if hasattr(best_model, \"decision_function\") else np.nan)\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"label\": y_pred,\n",
    "        \"phishing_probability\": float(y_proba) if np.isscalar(y_proba) else np.nan\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0324b492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'Random Forest', 'label': 'legitimate', 'phishing_probability': 0.3358815643420907}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khyal\\Desktop\\Phishing Detection System\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\khyal\\Desktop\\Phishing Detection System\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# new_features = {\n",
    "#     'length_url': 126, 'length_hostname': 50, 'ip': 1, 'nb_dots': 4, 'nb_hyphens': 1, 'nb_at': 0,\n",
    "#     'nb_qm': 1, 'nb_and': 2, 'nb_or': 0, 'nb_eq': 3, 'nb_underscore': 2, 'nb_tilde': 0, 'nb_percent': 0,\n",
    "#     'nb_slash': 5, 'nb_star': 0, 'nb_colon': 1, 'nb_comma': 0, 'nb_semicolumn': 0, 'nb_dollar': 0,\n",
    "#     'nb_space': 0, 'nb_www': 0, 'nb_com': 2, 'nb_dslash': 0, 'http_in_path': 0, 'https_token': 1,\n",
    "#     'ratio_digits_url': 0.15079365079365079, 'ratio_digits_host': 0.0, 'punycode': 0, 'port': 0,\n",
    "#     'tld_in_path': 0, 'tld_in_subdomain': 1, 'abnormal_subdomain': 1, 'nb_subdomains': 3, 'prefix_suffix': 0,\n",
    "#     'random_domain': 0, 'shortening_service': 0, 'path_extension': 0, 'nb_redirection': 0,\n",
    "#     'nb_external_redirection': 0, 'length_words_raw': 107, 'char_repeat': 2, 'shortest_words_raw': 2,\n",
    "#     'shortest_word_host': 50, 'shortest_word_path': 20, 'longest_words_raw': 17, 'longest_word_host': 50,\n",
    "#     'longest_word_path': 20, 'avg_words_raw': 7.642857142857143, 'avg_word_host': 50.0, 'avg_word_path': 20.0,\n",
    "#     'phish_hints': 1, 'domain_in_brand': 0, 'brand_in_subdomain': 1, 'brand_in_path': 0,\n",
    "#     'suspecious_tld': 0, 'statistical_report': 0\n",
    "# }\n",
    "new_features = feature_input  # Use the features extracted earlier\n",
    "result = predict_url_features(new_features)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c69599d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'Random Forest',\n",
       " 'label': 'legitimate',\n",
       " 'phishing_probability': 0.3358815643420907}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a470629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'legitimate', 'phishing_probability': 0.34}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_result = {\n",
    "    'label': result['label'],\n",
    "    'phishing_probability': round(result['phishing_probability'], 2)\n",
    "}\n",
    "required_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d4002",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'label': 'legitimate',\n",
    " 'phishing_probability': 0.33}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34b02af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'legitimate', 'phishing_probability': 0.34}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "0 i want to reduce latency - i will look for more efficient libraries\n",
    "1 additional features - scraping\n",
    "2 additional model training - based on new feature we will do ensamle modeling\n",
    "3 look for existing datasets with all phishing urls and legit urls but might have different features. then retrain model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae326efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478cc36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c700f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb04f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b12918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899327c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8f78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd2958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2f4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f810c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
